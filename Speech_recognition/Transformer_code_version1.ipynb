{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Transformer-based Speech Recognition with Noise Augmentation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data directory exists\n",
    "os.makedirs(\"./data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Noise Augmentation --------------------\n",
    "class AddGaussianNoise(nn.Module):\n",
    "    def __init__(self, noise_level=0.005):\n",
    "        super().__init__()\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(waveform) * self.noise_level\n",
    "            return waveform + noise\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Audio Transforms --------------------\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    AddGaussianNoise(0.01),\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Text Processing --------------------\n",
    "class TextTransform:\n",
    "    def __init__(self):\n",
    "        self.chars = [\"'\", '<SPACE>'] + list(string.ascii_lowercase)\n",
    "        self.char_map = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.index_map = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.index_map[self.char_map['<SPACE>']] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        return [self.char_map.get(c, self.char_map['<SPACE>']) for c in text.lower()]\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        return ''.join([self.index_map[i] for i in labels]).replace('<SPACE>', ' ')\n",
    "\n",
    "text_transform = TextTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Data Processing --------------------\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    specs, labels = [], []\n",
    "    input_lengths, label_lengths = [], []\n",
    "    transform = train_audio_transforms if data_type == 'train' else valid_audio_transforms\n",
    "    \n",
    "    for (waveform, _, utterance, *_ ) in data:\n",
    "        spec = transform(waveform).squeeze(0).transpose(0, 1)\n",
    "        specs.append(spec)\n",
    "        label = torch.tensor(text_transform.text_to_int(utterance))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0] // 2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    specs = nn.utils.rnn.pad_sequence(specs, batch_first=True)\n",
    "    specs = specs.unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    return specs, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Model Components --------------------\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    def __init__(self, n_feats):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2, 3).contiguous()\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel, stride, dropout, n_feats):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_ch, out_ch, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_ch, out_ch, kernel, stride, padding=kernel//2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln1 = CNNLayerNorm(n_feats)\n",
    "        self.ln2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.ln1(x)\n",
    "        x = nn.GELU()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.ln2(x)\n",
    "        x = nn.GELU()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cnn2(x)\n",
    "        return x + residual\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Main Model --------------------\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, n_cnn_layers=3, n_rnn_layers=5, rnn_dim=512,\n",
    "                 n_class=29, n_feats=128, stride=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=1)\n",
    "        self.rescnn = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, 3, 1, dropout, n_feats//2)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.linear = nn.Linear(32 * (n_feats//2), rnn_dim)\n",
    "        self.pos_encoder = PositionalEncoding(rnn_dim, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=rnn_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=dropout,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_rnn_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim, rnn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn(x)\n",
    "        batch, ch, feat_dim, seq_len = x.size()\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = x.view(batch, seq_len, ch * feat_dim)\n",
    "        x = self.linear(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Create padding mask\n",
    "        max_len = x.size(0)\n",
    "        mask = torch.zeros(batch, max_len, dtype=torch.bool, device=x.device)\n",
    "        for i, length in enumerate(input_lengths):\n",
    "            if length < max_len:\n",
    "                mask[i, length:] = True\n",
    "\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Training Setup --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "params = {\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"n_cnn_layers\": 3,\n",
    "    \"n_rnn_layers\": 5,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": 29,\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "\n",
    "model = SpeechRecognitionModel(**{k: params[k] for k in [\n",
    "    'n_cnn_layers','n_rnn_layers','rnn_dim','n_class','n_feats','stride','dropout']\n",
    "}).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'])\n",
    "criterion = nn.CTCLoss(blank=28).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Training Utilities --------------------\n",
    "def decode(outputs):\n",
    "    _, preds = torch.max(outputs, dim=2)\n",
    "    return [text_transform.int_to_text(p.tolist()) for p in preds]\n",
    "\n",
    "def levenshtein_distance(a, b):\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    \n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i\n",
    "            else:\n",
    "                cost = 0 if a[i-1] == b[j-1] else 1\n",
    "                dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
    "    return dp[m][n]\n",
    "\n",
    "def wer(ref, hyp):\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    return levenshtein_distance(ref_words, hyp_words) / max(len(ref_words), 1)\n",
    "\n",
    "def cer(ref, hyp):\n",
    "    return levenshtein_distance(ref, hyp) / max(len(ref), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Training Execution --------------------\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (specs, labels, input_lens, label_lens) in enumerate(loader):\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(specs, input_lens)\n",
    "        outputs = F.log_softmax(outputs, dim=2).transpose(0, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels, input_lens, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(loader)} Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = total_cer = total_wer = 0\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, input_lens, label_lens in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs, input_lens)\n",
    "            outputs = F.log_softmax(outputs, dim=2).transpose(0, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels, input_lens, label_lens)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_texts = decode(outputs.transpose(0, 1))\n",
    "            true_texts = [text_transform.int_to_text(l.tolist()) for l in labels]\n",
    "            \n",
    "            for ref, hyp in zip(true_texts, pred_texts):\n",
    "                total_cer += cer(ref, hyp)\n",
    "                total_wer += wer(ref, hyp)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_cer = total_cer / len(loader.dataset)\n",
    "    avg_wer = total_wer / len(loader.dataset)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f} | CER: {avg_cer:.4f} | WER: {avg_wer:.4f}\")\n",
    "    return avg_loss, avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Main Execution --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "        root=\"./data\", url=\"train-clean-100\", download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "        root=\"./data\", url=\"test-clean\", download=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: data_processing(x, \"train\")\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        collate_fn=lambda x: data_processing(x, \"valid\")\n",
    "    )\n",
    "\n",
    "    best_wer = float('inf')\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{params['epochs']}\")\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_cer, val_wer = validate(model, test_loader, criterion, device)\n",
    "        \n",
    "        if val_wer < best_wer:\n",
    "            best_wer = val_wer\n",
    "            torch.save(model.state_dict(), \"best_model_transformer.pth\")\n",
    "            print(\"Saved new best model!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
